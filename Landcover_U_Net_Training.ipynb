{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heo-JuYeong/Data_Analysis_Team3_Project/blob/main/Landcover_U_Net_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Landcover_Unet_Colab.ipynb 형태 코드 내용 (Python 셀 기준)\n",
        "# ✅ 1. Google Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_yUQLAOHxxL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe1166c-c1c0-4ba0-eb71-7eb797cf7cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 2. 필요한 라이브러리 설치\n",
        "!pip install numpy==1.26.4 imagecodecs tifffile --force-reinstall"
      ],
      "metadata": {
        "id": "ZxYgKRbMxyrX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "963cac78-b666-42d1-feb4-ebe56505f67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting imagecodecs\n",
            "  Using cached imagecodecs-2025.3.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting tifffile\n",
            "  Using cached tifffile-2025.6.11-py3-none-any.whl.metadata (32 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached imagecodecs-2025.3.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.6 MB)\n",
            "Using cached tifffile-2025.6.11-py3-none-any.whl (230 kB)\n",
            "Installing collected packages: numpy, tifffile, imagecodecs\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: tifffile\n",
            "    Found existing installation: tifffile 2025.6.11\n",
            "    Uninstalling tifffile-2025.6.11:\n",
            "      Successfully uninstalled tifffile-2025.6.11\n",
            "  Attempting uninstall: imagecodecs\n",
            "    Found existing installation: imagecodecs 2025.3.30\n",
            "    Uninstalling imagecodecs-2025.3.30:\n",
            "      Successfully uninstalled imagecodecs-2025.3.30\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed imagecodecs-2025.3.30 numpy-1.26.4 tifffile-2025.6.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "295ec3ca4ccd4a9d9b2a29430f9916cd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "t2JklNMAcXIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 3. 라이브러리 임포트\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import imagecodecs\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tifffile import imread"
      ],
      "metadata": {
        "id": "G8h7S87Mx7t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 4. 데이터셋 자동 분할 함수\n",
        "def split_dataset(image_dir, mask_dir, output_base, split_ratio=(0.7, 0.2, 0.1)):\n",
        "    images = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
        "    masks = sorted(glob(os.path.join(mask_dir, '*.tif')))\n",
        "    assert len(images) == len(masks), \"이미지와 마스크 수 불일치\"\n",
        "\n",
        "    data = list(zip(images, masks))\n",
        "    random.shuffle(data)\n",
        "    n_total = len(data)\n",
        "    n_train = int(split_ratio[0] * n_total)\n",
        "    n_val = int(split_ratio[1] * n_total)\n",
        "\n",
        "    splits = {\n",
        "        'train': data[:n_train],\n",
        "        'val': data[n_train:n_train + n_val],\n",
        "        'test': data[n_train + n_val:]\n",
        "    }\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(output_base, 'images', split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_base, 'masks', split), exist_ok=True)\n",
        "        for img_path, mask_path in splits[split]:\n",
        "            shutil.copy(img_path, os.path.join(output_base, 'images', split, os.path.basename(img_path)))\n",
        "            shutil.copy(mask_path, os.path.join(output_base, 'masks', split, os.path.basename(mask_path)))"
      ],
      "metadata": {
        "id": "MQMLw8-xyBFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 클래스 수 정의\n",
        "NUM_CLASSES = 9\n",
        "CLASS_WEIGHTS = torch.tensor([\n",
        "    1.0,  # 건물\n",
        "    1.2,  # 주차장\n",
        "    1.5,  # 도로\n",
        "    1.0,  # 가로수\n",
        "    1.3,  # 논\n",
        "    1.1,  # 밭\n",
        "    0.9,  # 산림\n",
        "    1.0,  # 나지\n",
        "    0.8   # 비대상지\n",
        "], dtype=torch.float32)\n",
        "# ✅ U-Net 모델 정의 (순수 정의만, 인스턴스 생성 X)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def CBR(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        # 인코더\n",
        "        self.enc1 = CBR(3, 64)\n",
        "        self.enc2 = CBR(64, 128)\n",
        "        self.enc3 = CBR(128, 256)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # 디코더\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = CBR(256, 128)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = CBR(128, 64)\n",
        "\n",
        "        self.final = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)               # [B, 64, H, W]\n",
        "        e2 = self.enc2(self.pool(e1))   # [B, 128, H/2, W/2]\n",
        "        e3 = self.enc3(self.pool(e2))   # [B, 256, H/4, W/4]\n",
        "\n",
        "        d2 = self.up2(e3)               # [B, 128, H/2, W/2]\n",
        "        d2 = torch.cat([d2, e2], dim=1) # [B, 256, H/2, W/2]\n",
        "        d2 = self.dec2(d2)              # [B, 128, H/2, W/2]\n",
        "\n",
        "        d1 = self.up1(d2)               # [B, 64, H, W]\n",
        "        d1 = torch.cat([d1, e1], dim=1) # [B, 128, H, W]\n",
        "        d1 = self.dec1(d1)              # [B, 64, H, W]\n",
        "\n",
        "        out = self.final(d1)            # [B, n_classes, H, W]\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "U0Br8kg-yEm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tifffile import imread\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "# ✅ 클래스 매핑 정의\n",
        "LABEL_MAP = {\n",
        "    10: 0,   # 건물\n",
        "    20: 1,   # 주차장\n",
        "    30: 2,   # 도로\n",
        "    40: 3,   # 가로수\n",
        "    50: 4,   # 논\n",
        "    60: 5,   # 밭\n",
        "    70: 6,   # 산림\n",
        "    80: 7,   # 나지\n",
        "    100: 8   # 비대상지\n",
        "}\n",
        "\n",
        "NUM_CLASSES = 9\n",
        "CLASS_NAMES = [\n",
        "    'Building', 'Parking lot', 'Road', 'Tree line',\n",
        "    'Rice field', 'Crop field', 'Forest', 'Bare land', 'No-data'\n",
        "]\n",
        "\n",
        "# ✅ 마스크 리매핑 함수 (255: 무시 클래스)\n",
        "def remap_mask(mask_np):\n",
        "    remapped = np.full_like(mask_np, fill_value=255)\n",
        "    for src, dst in LABEL_MAP.items():\n",
        "        remapped[mask_np == src] = dst\n",
        "    return remapped\n",
        "\n",
        "# ✅ Dataset 클래스\n",
        "class LandCoverTifDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir):\n",
        "        self.image_paths = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
        "        self.mask_paths = sorted(glob(os.path.join(mask_dir, '*.tif')))\n",
        "        assert len(self.image_paths) == len(self.mask_paths), \"이미지와 마스크 수 불일치\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = imread(self.image_paths[idx])\n",
        "        mask = imread(self.mask_paths[idx])\n",
        "\n",
        "        # 이미지 전처리\n",
        "        if image.ndim == 2:\n",
        "            image = np.stack([image]*3, axis=-1)\n",
        "        elif image.ndim == 3:\n",
        "            if image.shape[2] in [3, 4]:  # HWC\n",
        "                image = image[:, :, :3]\n",
        "            elif image.shape[0] in [3, 4]:  # CHW\n",
        "                image = np.transpose(image[:3], (1, 2, 0))\n",
        "            else:\n",
        "                raise ValueError(f\"예상치 못한 이미지 shape: {image.shape}\")\n",
        "\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        image = torch.FloatTensor(image).permute(2, 0, 1)\n",
        "\n",
        "        # 마스크 리매핑 및 변환\n",
        "        mask = mask.astype(np.int64)\n",
        "        mask = remap_mask(mask)\n",
        "        mask = torch.LongTensor(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# ✅ 손실 함수 정의 (255는 무시)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)"
      ],
      "metadata": {
        "id": "QNVMCie5yISO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 7. 학습 루프\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if len(loader) == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    for img, mask in loader:\n",
        "        img, mask = img.to(device), mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img)\n",
        "        loss = criterion(out, mask)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (선택)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "\n",
        "        valid = mask != 255  # 무시할 픽셀 제외\n",
        "        correct += (pred[valid] == mask[valid]).sum().item()\n",
        "        total += valid.sum().item()\n",
        "\n",
        "    acc = 100. * correct / total if total > 0 else 0.0\n",
        "    return total_loss / len(loader), acc"
      ],
      "metadata": {
        "id": "6Dr76xtIyM4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1epg145hdgz",
        "outputId": "334c1c1b-51d8-40d6-e278-ea427bc15995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun 13 05:43:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             48W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKwnYzTlvyOd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "3c0c4638-d7d9-484e-ef4e-08e62331a028"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CLASS_WEIGHTS' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2825346242>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2825346242>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS_WEIGHTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CLASS_WEIGHTS' is not defined"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    base_dir = '/content/drive/MyDrive/Colab Notebooks/3조 프로젝트 폴더/landcover_dataset'\n",
        "    image_main = os.path.join(base_dir, 'images/main1')\n",
        "    mask_main = os.path.join(base_dir, 'masks/main')\n",
        "\n",
        "    image_dir = os.path.join(base_dir, 'images/train')\n",
        "    mask_dir = os.path.join(base_dir, 'masks/train')\n",
        "\n",
        "    batch_size = 4\n",
        "    lr = 1e-3\n",
        "    epochs = 50\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = LandCoverTifDataset(image_dir, mask_dir)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = UNet(NUM_CLASSES).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device), ignore_index=255)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        model.train()\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            valid = (masks != 255)\n",
        "            correct += (preds[valid] == masks[valid]).sum().item()\n",
        "            total += valid.sum().item()\n",
        "\n",
        "        avg_loss = running_loss / len(loader)\n",
        "        acc = (correct / total) * 100 if total > 0 else 0.0\n",
        "\n",
        "        print(f\"[{epoch+1:02d}/{epochs}] Loss: {avg_loss:.4f} | Acc: {acc:.2f}%\")\n",
        "\n",
        "    # ✅ 학습 종료 후 모델 저장\n",
        "    save_path = os.path.join(base_dir, 'landcover_unet_colab.pth')\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"✅ 모델 저장 완료: {save_path}\")\n",
        "\n",
        "# 실행\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ✅ device 정의\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ✅ 검증용 DataLoader 구성\n",
        "val_dataset = LandCoverTifDataset(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/3조 프로젝트 폴더/landcover_dataset/images/val',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/3조 프로젝트 폴더/landcover_dataset/masks/val'\n",
        ")\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# ✅ 모델 불러오기 및 평가 모드 설정\n",
        "model = UNet(NUM_CLASSES).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/3조 프로젝트 폴더/landcover_dataset/landcover_unet_colab.pth'))\n",
        "model.eval()\n",
        "\n",
        "# ✅ 정확도 계산\n",
        "val_correct = 0\n",
        "val_total = 0\n",
        "with torch.no_grad():\n",
        "    for val_img, val_mask in val_loader:\n",
        "        val_img, val_mask = val_img.to(device), val_mask.to(device)\n",
        "        val_out = model(val_img)\n",
        "        val_pred = torch.argmax(val_out, dim=1)\n",
        "        valid = (val_mask != 255)\n",
        "        val_correct += (val_pred[valid] == val_mask[valid]).sum().item()\n",
        "        val_total += valid.sum().item()\n",
        "\n",
        "val_acc = val_correct / val_total * 100 if val_total > 0 else 0.0\n",
        "print(f\"✅ Validation Accuracy: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "mk24FJZd6KB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터셋 불러오기\n",
        "test_dataset = LandCoverTifDataset(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/3조 프로젝트 폴더/landcover_dataset/images/test',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/3조 프로젝트 폴더/landcover_dataset/masks/test'\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# 모델 재사용 또는 재로딩\n",
        "model = UNet(NUM_CLASSES).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/3조 프로젝트 폴더/landcover_dataset/landcover_unet_colab.pth'))\n",
        "model.eval()\n",
        "\n",
        "# 테스트 정확도 계산\n",
        "test_correct, test_total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for test_img, test_mask in test_loader:\n",
        "        test_img, test_mask = test_img.to(device), test_mask.to(device)\n",
        "        out = model(test_img)\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        valid = (test_mask != 255)\n",
        "        test_correct += (pred[valid] == test_mask[valid]).sum().item()\n",
        "        test_total += valid.sum().item()\n",
        "\n",
        "test_acc = test_correct / test_total * 100 if test_total > 0 else 0.0\n",
        "print(f\"🎯 Test Accuracy: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "TRIMLNfq__uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tifffile import imread as tif_read\n",
        "from PIL import Image\n",
        "\n",
        "def predict_image(model, image_path, device):\n",
        "    model.eval()\n",
        "    ext = os.path.splitext(image_path)[-1].lower()\n",
        "\n",
        "    # 🔍 이미지 로딩 방식 선택\n",
        "    if ext in ['.tif', '.tiff']:\n",
        "        image = tif_read(image_path)\n",
        "    elif ext in ['.jpg', '.jpeg', '.png']:\n",
        "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
        "\n",
        "    # 🔧 채널 정렬\n",
        "    if image.ndim == 2:\n",
        "        image = np.stack([image]*3, axis=-1)\n",
        "    elif image.ndim == 3 and image.shape[0] in [3, 4]:\n",
        "        image = np.transpose(image[:3], (1, 2, 0))\n",
        "\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    tensor = torch.FloatTensor(image).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor)\n",
        "        pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "2n47QiDNBcWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# 클래스 정의\n",
        "CLASS_NAMES = ['건물', '주차장', '도로', '가로수', '논', '밭', '산림', '나지', '비대상지']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "base_cmap = cm.get_cmap('tab20')\n",
        "COLORS = [base_cmap(i / NUM_CLASSES) for i in range(NUM_CLASSES)]\n",
        "\n",
        "# 추론 함수\n",
        "def predict_image(model, image_path, device):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert('RGB').resize((256, 256))\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "    return pred_mask\n",
        "\n",
        "# 실행 예시\n",
        "image_path = '/content/drive/MyDrive/Colab Notebooks/3조 프로젝트 폴더/landcover_dataset/test1/002.jpg'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 모델은 이미 정의 및 로드되었다고 가정 (예: model.load_state_dict(...) 완료된 상태)\n",
        "pred_mask = predict_image(model, image_path, device)\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(8, 8))\n",
        "im = plt.imshow(pred_mask, cmap='tab20', vmin=0, vmax=NUM_CLASSES - 1)\n",
        "plt.title(\"예측된 분류 마스크\")\n",
        "plt.axis('off')\n",
        "handles = [mpatches.Patch(color=COLORS[i], label=CLASS_NAMES[i]) for i in range(NUM_CLASSES)]\n",
        "plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LRcUA8dtD_3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from tifffile import imread\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "counts = Counter()\n",
        "for path in glob('/content/drive/MyDrive/.../masks/train/*.tif'):\n",
        "    m = imread(path)\n",
        "    m_remap = remap_mask(m)\n",
        "    classes, class_counts = np.unique(m_remap, return_counts=True)\n",
        "    counts.update(dict(zip(classes, class_counts)))\n",
        "\n",
        "print(\"클래스 분포:\", counts)"
      ],
      "metadata": {
        "id": "zI2gP9s_Ua06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}