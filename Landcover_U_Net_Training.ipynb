{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heo-JuYeong/Data_Analysis_Team3_Project/blob/main/Landcover_U_Net_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Landcover_Unet_Colab.ipynb ÌòïÌÉú ÏΩîÎìú ÎÇ¥Ïö© (Python ÏÖÄ Í∏∞Ï§Ä)\n",
        "# ‚úÖ 1. Google Drive ÎßàÏö¥Ìä∏\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_yUQLAOHxxL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe1166c-c1c0-4ba0-eb71-7eb797cf7cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 2. ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
        "!pip install numpy==1.26.4 imagecodecs tifffile --force-reinstall"
      ],
      "metadata": {
        "id": "ZxYgKRbMxyrX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "963cac78-b666-42d1-feb4-ebe56505f67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting imagecodecs\n",
            "  Using cached imagecodecs-2025.3.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting tifffile\n",
            "  Using cached tifffile-2025.6.11-py3-none-any.whl.metadata (32 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached imagecodecs-2025.3.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.6 MB)\n",
            "Using cached tifffile-2025.6.11-py3-none-any.whl (230 kB)\n",
            "Installing collected packages: numpy, tifffile, imagecodecs\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: tifffile\n",
            "    Found existing installation: tifffile 2025.6.11\n",
            "    Uninstalling tifffile-2025.6.11:\n",
            "      Successfully uninstalled tifffile-2025.6.11\n",
            "  Attempting uninstall: imagecodecs\n",
            "    Found existing installation: imagecodecs 2025.3.30\n",
            "    Uninstalling imagecodecs-2025.3.30:\n",
            "      Successfully uninstalled imagecodecs-2025.3.30\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed imagecodecs-2025.3.30 numpy-1.26.4 tifffile-2025.6.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "295ec3ca4ccd4a9d9b2a29430f9916cd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "t2JklNMAcXIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 3. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import imagecodecs\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tifffile import imread"
      ],
      "metadata": {
        "id": "G8h7S87Mx7t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 4. Îç∞Ïù¥ÌÑ∞ÏÖã ÏûêÎèô Î∂ÑÌï† Ìï®Ïàò\n",
        "def split_dataset(image_dir, mask_dir, output_base, split_ratio=(0.7, 0.2, 0.1)):\n",
        "    images = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
        "    masks = sorted(glob(os.path.join(mask_dir, '*.tif')))\n",
        "    assert len(images) == len(masks), \"Ïù¥ÎØ∏ÏßÄÏôÄ ÎßàÏä§ÌÅ¨ Ïàò Î∂àÏùºÏπò\"\n",
        "\n",
        "    data = list(zip(images, masks))\n",
        "    random.shuffle(data)\n",
        "    n_total = len(data)\n",
        "    n_train = int(split_ratio[0] * n_total)\n",
        "    n_val = int(split_ratio[1] * n_total)\n",
        "\n",
        "    splits = {\n",
        "        'train': data[:n_train],\n",
        "        'val': data[n_train:n_train + n_val],\n",
        "        'test': data[n_train + n_val:]\n",
        "    }\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(output_base, 'images', split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_base, 'masks', split), exist_ok=True)\n",
        "        for img_path, mask_path in splits[split]:\n",
        "            shutil.copy(img_path, os.path.join(output_base, 'images', split, os.path.basename(img_path)))\n",
        "            shutil.copy(mask_path, os.path.join(output_base, 'masks', split, os.path.basename(mask_path)))"
      ],
      "metadata": {
        "id": "MQMLw8-xyBFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ÌÅ¥ÎûòÏä§ Ïàò Ï†ïÏùò\n",
        "NUM_CLASSES = 9\n",
        "CLASS_WEIGHTS = torch.tensor([\n",
        "    1.0,  # Í±¥Î¨º\n",
        "    1.2,  # Ï£ºÏ∞®Ïû•\n",
        "    1.5,  # ÎèÑÎ°ú\n",
        "    1.0,  # Í∞ÄÎ°úÏàò\n",
        "    1.3,  # ÎÖº\n",
        "    1.1,  # Î∞≠\n",
        "    0.9,  # ÏÇ∞Î¶º\n",
        "    1.0,  # ÎÇòÏßÄ\n",
        "    0.8   # ÎπÑÎåÄÏÉÅÏßÄ\n",
        "], dtype=torch.float32)\n",
        "# ‚úÖ U-Net Î™®Îç∏ Ï†ïÏùò (ÏàúÏàò Ï†ïÏùòÎßå, Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± X)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def CBR(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        # Ïù∏ÏΩîÎçî\n",
        "        self.enc1 = CBR(3, 64)\n",
        "        self.enc2 = CBR(64, 128)\n",
        "        self.enc3 = CBR(128, 256)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # ÎîîÏΩîÎçî\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = CBR(256, 128)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = CBR(128, 64)\n",
        "\n",
        "        self.final = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)               # [B, 64, H, W]\n",
        "        e2 = self.enc2(self.pool(e1))   # [B, 128, H/2, W/2]\n",
        "        e3 = self.enc3(self.pool(e2))   # [B, 256, H/4, W/4]\n",
        "\n",
        "        d2 = self.up2(e3)               # [B, 128, H/2, W/2]\n",
        "        d2 = torch.cat([d2, e2], dim=1) # [B, 256, H/2, W/2]\n",
        "        d2 = self.dec2(d2)              # [B, 128, H/2, W/2]\n",
        "\n",
        "        d1 = self.up1(d2)               # [B, 64, H, W]\n",
        "        d1 = torch.cat([d1, e1], dim=1) # [B, 128, H, W]\n",
        "        d1 = self.dec1(d1)              # [B, 64, H, W]\n",
        "\n",
        "        out = self.final(d1)            # [B, n_classes, H, W]\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "U0Br8kg-yEm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tifffile import imread\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "# ‚úÖ ÌÅ¥ÎûòÏä§ Îß§Ìïë Ï†ïÏùò\n",
        "LABEL_MAP = {\n",
        "    10: 0,   # Í±¥Î¨º\n",
        "    20: 1,   # Ï£ºÏ∞®Ïû•\n",
        "    30: 2,   # ÎèÑÎ°ú\n",
        "    40: 3,   # Í∞ÄÎ°úÏàò\n",
        "    50: 4,   # ÎÖº\n",
        "    60: 5,   # Î∞≠\n",
        "    70: 6,   # ÏÇ∞Î¶º\n",
        "    80: 7,   # ÎÇòÏßÄ\n",
        "    100: 8   # ÎπÑÎåÄÏÉÅÏßÄ\n",
        "}\n",
        "\n",
        "NUM_CLASSES = 9\n",
        "CLASS_NAMES = [\n",
        "    'Building', 'Parking lot', 'Road', 'Tree line',\n",
        "    'Rice field', 'Crop field', 'Forest', 'Bare land', 'No-data'\n",
        "]\n",
        "\n",
        "# ‚úÖ ÎßàÏä§ÌÅ¨ Î¶¨Îß§Ìïë Ìï®Ïàò (255: Î¨¥Ïãú ÌÅ¥ÎûòÏä§)\n",
        "def remap_mask(mask_np):\n",
        "    remapped = np.full_like(mask_np, fill_value=255)\n",
        "    for src, dst in LABEL_MAP.items():\n",
        "        remapped[mask_np == src] = dst\n",
        "    return remapped\n",
        "\n",
        "# ‚úÖ Dataset ÌÅ¥ÎûòÏä§\n",
        "class LandCoverTifDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir):\n",
        "        self.image_paths = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
        "        self.mask_paths = sorted(glob(os.path.join(mask_dir, '*.tif')))\n",
        "        assert len(self.image_paths) == len(self.mask_paths), \"Ïù¥ÎØ∏ÏßÄÏôÄ ÎßàÏä§ÌÅ¨ Ïàò Î∂àÏùºÏπò\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = imread(self.image_paths[idx])\n",
        "        mask = imread(self.mask_paths[idx])\n",
        "\n",
        "        # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨\n",
        "        if image.ndim == 2:\n",
        "            image = np.stack([image]*3, axis=-1)\n",
        "        elif image.ndim == 3:\n",
        "            if image.shape[2] in [3, 4]:  # HWC\n",
        "                image = image[:, :, :3]\n",
        "            elif image.shape[0] in [3, 4]:  # CHW\n",
        "                image = np.transpose(image[:3], (1, 2, 0))\n",
        "            else:\n",
        "                raise ValueError(f\"ÏòàÏÉÅÏπò Î™ªÌïú Ïù¥ÎØ∏ÏßÄ shape: {image.shape}\")\n",
        "\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        image = torch.FloatTensor(image).permute(2, 0, 1)\n",
        "\n",
        "        # ÎßàÏä§ÌÅ¨ Î¶¨Îß§Ìïë Î∞è Î≥ÄÌôò\n",
        "        mask = mask.astype(np.int64)\n",
        "        mask = remap_mask(mask)\n",
        "        mask = torch.LongTensor(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# ‚úÖ ÏÜêÏã§ Ìï®Ïàò Ï†ïÏùò (255Îäî Î¨¥Ïãú)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)"
      ],
      "metadata": {
        "id": "QNVMCie5yISO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 7. ÌïôÏäµ Î£®ÌîÑ\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if len(loader) == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    for img, mask in loader:\n",
        "        img, mask = img.to(device), mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img)\n",
        "        loss = criterion(out, mask)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (ÏÑ†ÌÉù)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "\n",
        "        valid = mask != 255  # Î¨¥ÏãúÌï† ÌîΩÏÖÄ Ï†úÏô∏\n",
        "        correct += (pred[valid] == mask[valid]).sum().item()\n",
        "        total += valid.sum().item()\n",
        "\n",
        "    acc = 100. * correct / total if total > 0 else 0.0\n",
        "    return total_loss / len(loader), acc"
      ],
      "metadata": {
        "id": "6Dr76xtIyM4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1epg145hdgz",
        "outputId": "334c1c1b-51d8-40d6-e278-ea427bc15995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun 13 05:43:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             48W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKwnYzTlvyOd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "3c0c4638-d7d9-484e-ef4e-08e62331a028"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CLASS_WEIGHTS' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2825346242>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Ïã§Ìñâ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2825346242>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS_WEIGHTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CLASS_WEIGHTS' is not defined"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    base_dir = '/content/drive/MyDrive/Colab Notebooks/3Ï°∞ ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî/landcover_dataset'\n",
        "    image_main = os.path.join(base_dir, 'images/main1')\n",
        "    mask_main = os.path.join(base_dir, 'masks/main')\n",
        "\n",
        "    image_dir = os.path.join(base_dir, 'images/train')\n",
        "    mask_dir = os.path.join(base_dir, 'masks/train')\n",
        "\n",
        "    batch_size = 4\n",
        "    lr = 1e-3\n",
        "    epochs = 50\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = LandCoverTifDataset(image_dir, mask_dir)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = UNet(NUM_CLASSES).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device), ignore_index=255)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        model.train()\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            valid = (masks != 255)\n",
        "            correct += (preds[valid] == masks[valid]).sum().item()\n",
        "            total += valid.sum().item()\n",
        "\n",
        "        avg_loss = running_loss / len(loader)\n",
        "        acc = (correct / total) * 100 if total > 0 else 0.0\n",
        "\n",
        "        print(f\"[{epoch+1:02d}/{epochs}] Loss: {avg_loss:.4f} | Acc: {acc:.2f}%\")\n",
        "\n",
        "    # ‚úÖ ÌïôÏäµ Ï¢ÖÎ£å ÌõÑ Î™®Îç∏ Ï†ÄÏû•\n",
        "    save_path = os.path.join(base_dir, 'landcover_unet_colab.pth')\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"‚úÖ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: {save_path}\")\n",
        "\n",
        "# Ïã§Ìñâ\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ‚úÖ device Ï†ïÏùò\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ‚úÖ Í≤ÄÏ¶ùÏö© DataLoader Íµ¨ÏÑ±\n",
        "val_dataset = LandCoverTifDataset(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/3Ï°∞ ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî/landcover_dataset/images/val',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/3Ï°∞ ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî/landcover_dataset/masks/val'\n",
        ")\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# ‚úÖ Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞ Î∞è ÌèâÍ∞Ä Î™®Îìú ÏÑ§Ï†ï\n",
        "model = UNet(NUM_CLASSES).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/3Ï°∞ ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî/landcover_dataset/landcover_unet_colab.pth'))\n",
        "model.eval()\n",
        "\n",
        "# ‚úÖ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞\n",
        "val_correct = 0\n",
        "val_total = 0\n",
        "with torch.no_grad():\n",
        "    for val_img, val_mask in val_loader:\n",
        "        val_img, val_mask = val_img.to(device), val_mask.to(device)\n",
        "        val_out = model(val_img)\n",
        "        val_pred = torch.argmax(val_out, dim=1)\n",
        "        valid = (val_mask != 255)\n",
        "        val_correct += (val_pred[valid] == val_mask[valid]).sum().item()\n",
        "        val_total += valid.sum().item()\n",
        "\n",
        "val_acc = val_correct / val_total * 100 if val_total > 0 else 0.0\n",
        "print(f\"‚úÖ Validation Accuracy: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "mk24FJZd6KB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã Î∂àÎü¨Ïò§Í∏∞\n",
        "test_dataset = LandCoverTifDataset(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/3Ï°∞ ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî/landcover_dataset/images/test',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/3Ï°∞ ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî/landcover_dataset/masks/test'\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Î™®Îç∏ Ïû¨ÏÇ¨Ïö© ÎòêÎäî Ïû¨Î°úÎî©\n",
        "model = UNet(NUM_CLASSES).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/3Ï°∞ ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî/landcover_dataset/landcover_unet_colab.pth'))\n",
        "model.eval()\n",
        "\n",
        "# ÌÖåÏä§Ìä∏ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞\n",
        "test_correct, test_total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for test_img, test_mask in test_loader:\n",
        "        test_img, test_mask = test_img.to(device), test_mask.to(device)\n",
        "        out = model(test_img)\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        valid = (test_mask != 255)\n",
        "        test_correct += (pred[valid] == test_mask[valid]).sum().item()\n",
        "        test_total += valid.sum().item()\n",
        "\n",
        "test_acc = test_correct / test_total * 100 if test_total > 0 else 0.0\n",
        "print(f\"üéØ Test Accuracy: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "TRIMLNfq__uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tifffile import imread as tif_read\n",
        "from PIL import Image\n",
        "\n",
        "def predict_image(model, image_path, device):\n",
        "    model.eval()\n",
        "    ext = os.path.splitext(image_path)[-1].lower()\n",
        "\n",
        "    # üîç Ïù¥ÎØ∏ÏßÄ Î°úÎî© Î∞©Ïãù ÏÑ†ÌÉù\n",
        "    if ext in ['.tif', '.tiff']:\n",
        "        image = tif_read(image_path)\n",
        "    elif ext in ['.jpg', '.jpeg', '.png']:\n",
        "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
        "\n",
        "    # üîß Ï±ÑÎÑê Ï†ïÎ†¨\n",
        "    if image.ndim == 2:\n",
        "        image = np.stack([image]*3, axis=-1)\n",
        "    elif image.ndim == 3 and image.shape[0] in [3, 4]:\n",
        "        image = np.transpose(image[:3], (1, 2, 0))\n",
        "\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    tensor = torch.FloatTensor(image).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor)\n",
        "        pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "2n47QiDNBcWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
        "CLASS_NAMES = ['Í±¥Î¨º', 'Ï£ºÏ∞®Ïû•', 'ÎèÑÎ°ú', 'Í∞ÄÎ°úÏàò', 'ÎÖº', 'Î∞≠', 'ÏÇ∞Î¶º', 'ÎÇòÏßÄ', 'ÎπÑÎåÄÏÉÅÏßÄ']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "base_cmap = cm.get_cmap('tab20')\n",
        "COLORS = [base_cmap(i / NUM_CLASSES) for i in range(NUM_CLASSES)]\n",
        "\n",
        "# Ï∂îÎ°† Ìï®Ïàò\n",
        "def predict_image(model, image_path, device):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert('RGB').resize((256, 256))\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "    return pred_mask\n",
        "\n",
        "# Ïã§Ìñâ ÏòàÏãú\n",
        "image_path = '/content/drive/MyDrive/Colab Notebooks/3Ï°∞ ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî/landcover_dataset/test1/002.jpg'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Î™®Îç∏ÏùÄ Ïù¥ÎØ∏ Ï†ïÏùò Î∞è Î°úÎìúÎêòÏóàÎã§Í≥† Í∞ÄÏ†ï (Ïòà: model.load_state_dict(...) ÏôÑÎ£åÎêú ÏÉÅÌÉú)\n",
        "pred_mask = predict_image(model, image_path, device)\n",
        "\n",
        "# ÏãúÍ∞ÅÌôî\n",
        "plt.figure(figsize=(8, 8))\n",
        "im = plt.imshow(pred_mask, cmap='tab20', vmin=0, vmax=NUM_CLASSES - 1)\n",
        "plt.title(\"ÏòàÏ∏°Îêú Î∂ÑÎ•ò ÎßàÏä§ÌÅ¨\")\n",
        "plt.axis('off')\n",
        "handles = [mpatches.Patch(color=COLORS[i], label=CLASS_NAMES[i]) for i in range(NUM_CLASSES)]\n",
        "plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LRcUA8dtD_3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from tifffile import imread\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "counts = Counter()\n",
        "for path in glob('/content/drive/MyDrive/.../masks/train/*.tif'):\n",
        "    m = imread(path)\n",
        "    m_remap = remap_mask(m)\n",
        "    classes, class_counts = np.unique(m_remap, return_counts=True)\n",
        "    counts.update(dict(zip(classes, class_counts)))\n",
        "\n",
        "print(\"ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨:\", counts)"
      ],
      "metadata": {
        "id": "zI2gP9s_Ua06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}